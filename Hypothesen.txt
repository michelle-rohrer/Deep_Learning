Kategorie 1: Modellkomplexität
Hypothese 1: Modelltiefe (Anzahl Conv-Layer)
a) Hypothese:
Erwartung: Mehr Conv-Layer erhöhen die Modellkapazität und können komplexere Features lernen, was die Performance verbessert. Zu viele Layer können jedoch zu Overfitting führen und das Training erschweren (Vanishing Gradient Problem).
Begründung: Tiefere Netzwerke können hierarchische Features lernen (shallow → deep: edges → patterns → objects). Referenz: He et al. (2016) "Deep Residual Learning for Image Recognition"
Erwartung: 4-5 Conv-Layer sollten besser sein als 3, aber >6 Layer könnten ohne Residual Connections Probleme haben.
b) Experiment:
Baseline: 3 Conv-Layer (aktuell)
Varianten: 2, 4, 5, 6 Conv-Layer
Alle anderen Parameter gleich (LR=0.01, Batch=64, 60 Epochen)
c) Analyse:
Vergleich von Train/Val Accuracy, Loss, F1-Score
Overfitting-Gap analysieren
Training-Stabilität prüfen

Hypothese 2: Modellbreite (Anzahl Filter)
a) Hypothese:
Erwartung: Mehr Filter pro Layer erhöhen die Kapazität und können mehr Features gleichzeitig lernen. Dies verbessert die Performance, erhöht aber auch die Parameteranzahl und das Overfitting-Risiko.
Begründung: Mehr Filter ermöglichen mehr Feature-Maps. Referenz: Simonyan & Zisserman (2014) "Very Deep Convolutional Networks"
Erwartung: 32/64/128 Filter sollten besser sein als 16/32/64, aber mit höherem Overfitting-Risiko.
b) Experiment:
Baseline: [16, 32, 64] Filter
Varianten: [8, 16, 32], [32, 64, 128], [64, 128, 256]
c) Analyse:
Performance vs. Parameteranzahl
Overfitting-Verhalten

Hypothese 3: FC-Layer Breite
a) Hypothese:
Erwartung: Mehr Neuronen in den FC-Layern können komplexere Entscheidungsgrenzen lernen, aber zu viele Neuronen führen zu Overfitting.
Begründung: FC-Layer kombinieren Features für die finale Klassifikation. Referenz: Goodfellow et al. (2016) "Deep Learning"
Erwartung: 128-256 Neuronen sollten besser sein als 64, aber >512 könnte Overfitting verstärken.
b) Experiment:
Baseline: 64 Neuronen in fc1
Varianten: 32, 128, 256, 512 Neuronen
c) Analyse:
Performance vs. FC-Layer-Größe
Overfitting-Gap



Kategorie 2: Conv-Layer Settings
Hypothese 4: Kernel Size
a) Hypothese:
Erwartung: Größere Kernel (5×5, 7×7) erfassen größere Receptive Fields, können aber mehr Parameter haben. Kleinere Kernel (3×3) sind effizienter und können durch Stacking ähnliche Receptive Fields erreichen.
Begründung: 3×3 Kernel sind effizienter als größere Kernel. Referenz: Simonyan & Zisserman (2014)
Erwartung: 3×3 sollte optimal sein; 5×5 könnte ähnlich performen, aber ineffizienter sein.
b) Experiment:
Baseline: 3×3 Kernel
Varianten: 5×5, 7×7 Kernel (mit entsprechendem Padding)
c) Analyse:
Performance vs. Parameteranzahl
Training-Zeit

Hypothese 5: Pooling-Strategie
a) Hypothese:
Erwartung: Average Pooling ist robuster gegen Noise, Max Pooling betont starke Features. Adaptive Pooling könnte flexibler sein.
Begründung: Max Pooling ist Standard, aber Average Pooling kann in manchen Fällen besser generalisieren. Referenz: Goodfellow et al. (2016)
Erwartung: Max Pooling sollte besser sein, aber Average Pooling könnte weniger Overfitting zeigen.
b) Experiment:
Baseline: MaxPool2d(2×2)
Varianten: AvgPool2d(2×2), AdaptiveAvgPool2d
c) Analyse:
Generalisierung
Feature-Qualität



Kategorie 3: Regularisierung
Hypothese 6: Dropout
a) Hypothese:
Erwartung: Dropout reduziert Overfitting durch zufälliges Deaktivieren von Neuronen während des Trainings, was die Generalisierung verbessert. Zu hohe Dropout-Raten können das Training erschweren.
Begründung: Dropout verhindert Co-Adaptation von Neuronen. Referenz: Srivastava et al. (2014) "Dropout: A Simple Way to Prevent Neural Networks from Overfitting"
Erwartung: Dropout 0.3-0.5 sollte Overfitting reduzieren und Val-Accuracy verbessern.
b) Experiment:
Baseline: Kein Dropout
Varianten: Dropout(0.2), Dropout(0.3), Dropout(0.5) nach Conv-Layern und/oder FC-Layern
c) Analyse:
Train-Val Gap
Finale Val-Accuracy

Hypothese 7: Data Augmentation
a) Hypothese:
Erwartung: Data Augmentation (Rotation, Translation, Flip) erhöht die Datenvielfalt und verbessert die Generalisierung, reduziert Overfitting.
Begründung: Mehr Variation im Training macht das Modell robuster. Referenz: Shorten & Khoshgoftaar (2019) "A survey on Image Data Augmentation"
Erwartung: Moderate Augmentation sollte die Val-Accuracy verbessern, zu aggressive Augmentation könnte das Training erschweren.
b) Experiment:
Baseline: Keine Augmentation
Varianten:
Rotation (±10°)
Horizontal Flip
Translation (±10%)
Kombination
c) Analyse:
Generalisierung
Train-Val Gap



Kategorie 4: BatchNorm
Hypothese 8: BatchNorm
a) Hypothese:
Erwartung: BatchNorm normalisiert Aktivierungen, beschleunigt das Training, ermöglicht höhere Lernraten und reduziert Overfitting durch interne Covariate Shift-Reduktion.
Begründung: BatchNorm stabilisiert das Training. Referenz: Ioffe & Szegedy (2015) "Batch Normalization: Accelerating Deep Network Training"
Erwartung: BatchNorm sollte Training beschleunigen und Performance verbessern.
b) Experiment:
Baseline: Kein BatchNorm
Varianten: BatchNorm nach jedem Conv-Layer
c) Analyse:
Training-Geschwindigkeit
Konvergenz-Verhalten
Finale Performance



Kategorie 5: Initialisierung
Hypothese 9: Gewichts-Initialisierung
a) Hypothese:
Erwartung: He/Kaiming Initialization ist für ReLU-Aktivierungen optimal, Xavier/Glorot für tanh/sigmoid. Bessere Initialisierung beschleunigt Konvergenz.
Begründung: Richtige Initialisierung verhindert Vanishing/Exploding Gradients. Referenz: He et al. (2015) "Delving Deep into Rectifiers"
Erwartung: He Initialization sollte besser sein als Default (PyTorch verwendet bereits Kaiming für Conv).
b) Experiment:
Baseline: Default PyTorch (bereits Kaiming für Conv)
Varianten: Explizite He/Kaiming Init, Xavier Init, Zufällige kleine Werte
c) Analyse:
Konvergenz-Geschwindigkeit
Training-Stabilität



Kategorie 6: Optimizer
Hypothese 10: Adam Optimizer
a) Hypothese:
Erwartung: Adam passt die Lernrate adaptiv an, konvergiert oft schneller als SGD, kann aber zu schlechterer Generalisierung führen (höheres Overfitting).
Begründung: Adam kombiniert Momentum und adaptive Lernraten. Referenz: Kingma & Ba (2014) "Adam: A Method for Stochastic Optimization"
Erwartung: Adam sollte schneller konvergieren, aber möglicherweise höheres Overfitting zeigen.
b) Experiment:
Baseline: SGD (momentum=0, LR=0.01)
Varianten: Adam (LR=0.001), Adam (LR=0.0001), SGD mit Momentum (0.9)
c) Analyse:
Konvergenz-Geschwindigkeit
Finale Performance
Overfitting-Verhalten



Kategorie 7: Transfer Learning
Hypothese 11: Transfer Learning (Pre-trained)
a) Hypothese:
Erwartung: Pre-trained Modelle (z.B. auf ImageNet) können durch Transfer Learning bessere Performance erreichen, besonders bei kleinen Datensätzen.
Begründung: Pre-trained Features sind generalisierbar. Referenz: Yosinski et al. (2014) "How transferable are features in deep neural networks?"
Erwartung: Transfer Learning sollte deutlich bessere Performance zeigen, besonders bei Feature Extraction.
b) Experiment:
Baseline: Training from Scratch
Varianten:
Pre-trained ResNet18 (Feature Extraction)
Pre-trained ResNet18 (Fine-tuning)
c) Analyse:
Performance-Verbesserung
Training-Zeit
Daten-Effizienz

Hypothese 12: Kombination (Dropout + BatchNorm + Data Augmentation)
a) Hypothese:
Erwartung: Kombination mehrerer Regularisierungstechniken sollte Overfitting stärker reduzieren und Generalisierung verbessern.
Begründung: Verschiedene Regularisierungstechniken wirken komplementär. Referenz: Goodfellow et al. (2016)
Erwartung: Kombination sollte beste Generalisierung zeigen.
b) Experiment:
Baseline: Keine Regularisierung
Varianten: Dropout(0.3) + BatchNorm + Data Augmentation
c) Analyse:
Train-Val Gap
Finale Val-Accuracy
