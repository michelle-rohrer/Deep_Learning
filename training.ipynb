{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deep Learning Challenge - Training\n",
        "## Michelle Rohrer\n",
        "\n",
        "Dieses Notebook enthält alle Trainings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pakete laden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WanDB Key erfolgreich geladen\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=UserWarning, module='pydantic')\n",
        "\n",
        "# Dann der Rest Ihrer Imports\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import wandb\n",
        "from dotenv import load_dotenv\n",
        "import pickle\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchsummary import summary\n",
        "\n",
        "from src.plots import (\n",
        "    plot_training_curves\n",
        ")\n",
        "from src.model import BaselineCNN\n",
        "from src.test_train import (\n",
        "    overfitting_test_batch,\n",
        "    train_model,\n",
        "    hyperparameter_tuning_with_wandb\n",
        ")\n",
        "from src.evaluation import (\n",
        "    cross_validation_training\n",
        ")\n",
        "\n",
        "load_dotenv()\n",
        "wandb_key = os.getenv('KEY')\n",
        "\n",
        "if wandb_key:\n",
        "    print(\"WanDB Key erfolgreich geladen\")\n",
        "else:\n",
        "    print(\"WanDB Key nicht gefunden\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Daten laden\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# .pkl laden\n",
        "with open('data/train_dataset.pkl', 'rb') as f:\n",
        "    train_dataset = pickle.load(f)\n",
        "\n",
        "with open('data/val_dataset.pkl', 'rb') as f:\n",
        "    val_dataset = pickle.load(f)\n",
        "\n",
        "with open('data/test_dataset.pkl', 'rb') as f:\n",
        "    test_dataset = pickle.load(f)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training des Basismodells\n",
        "\n",
        "#### Weights & Biases (wandb) Integration\n",
        "\n",
        "**Zweck:** Automatisches Tracking aller Experimente für bessere Reproduzierbarkeit und Analyse.\n",
        "\n",
        "**Was wird getrackt:**\n",
        "- **Hyperparameter:** Lernrate, Batch-Größe, Optimizer-Einstellungen\n",
        "- **Metriken:** Training/Validation Loss und Accuracy pro Epoche\n",
        "- **Test-Metriken:** Accuracy, Precision, Recall, F1-Score, Top-K Accuracy\n",
        "- **Konfusionsmatrix:** Visualisierung der Klassifikationsergebnisse\n",
        "- **Lernkurven:** Automatische Plots für alle Konfigurationen\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# wandb Setup\n",
        "# wandb API Key aus .env laden\n",
        "wandb_key = os.getenv('KEY')\n",
        "if wandb_key:\n",
        "    wandb.login(key=wandb_key)\n",
        "    print(\"wandb erfolgreich authentifiziert\")\n",
        "else:\n",
        "    print(\"wandb API Key nicht in .env gefunden\")\n",
        "\n",
        "# Device Setup\n",
        "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "print(f\"Verwende Device: {device}\")\n",
        "\n",
        "# Klassennamen für Evaluation\n",
        "class_names = list(full_train_dataset.class_to_idx.keys())\n",
        "print(f\"Klassen: {class_names}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basismodell erstellen und trainieren\n",
        "print(\"=== Training des Basismodells ===\")\n",
        "print(\"Konfiguration: SGD (momentum=0), LR=0.01, Batch=64, 200 Epochen\")\n",
        "\n",
        "# Modell initialisieren\n",
        "model = BaselineCNN(img_size=img_size, num_classes=num_classes).to(device)\n",
        "\n",
        "# Training durchführen mit wandb\n",
        "train_losses, val_losses, train_accs, val_accs = train_model(\n",
        "    model=model,\n",
        "    device=device,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    num_epochs=200,\n",
        "    learning_rate=0.01,\n",
        "    batch_size=64,\n",
        "    use_wandb=True,\n",
        "    run_name=\"baseline_model\",\n",
        "    early_stopping=True,\n",
        "    patience=15,  # Stoppe wenn 15 Epochen ohne Verbesserung\n",
        "    min_delta=0.001\n",
        ")\n",
        "\n",
        "# Lernkurven plotten\n",
        "plot_training_curves(train_losses, val_losses, train_accs, val_accs, \n",
        "                    title=\"Basismodell: SGD (momentum=0), LR=0.01, Batch=64\")\n",
        "\n",
        "# Modell speichern\n",
        "torch.save(model.state_dict(), 'models/baseline_model.pth')\n",
        "print(\"Basismodell gespeichert als 'models/baseline_model.pth'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hyperparameter-Tuning\n",
        "\n",
        "**Ziel:** Vergleich verschiedener Lernraten und Batch-Größen anhand der Lernkurven und Metriken.\n",
        "\n",
        "**Testkonfigurationen:**\n",
        "- **Lernraten:** [0.001, 0.01, 0.1]\n",
        "- **Batch-Größen:** [32, 64, 128]\n",
        "- **Epochen:** 100 (für schnelleren Vergleich)\n",
        "- **Optimizer:** SGD ohne Momentum\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameter-Tuning mit wandb Integration\n",
        "print(\"=== Hyperparameter-Tuning mit wandb ===\")\n",
        "\n",
        "# Testkonfigurationen\n",
        "learning_rates = [0.001, 0.01, 0.1]\n",
        "batch_sizes = [16, 32, 64]  # Kleinere Batches für bessere Performance\n",
        "num_epochs = 50  # Weniger Epochen für schnellere Tests\n",
        "\n",
        "# Hyperparameter-Tuning mit wandb durchführen\n",
        "results = hyperparameter_tuning_with_wandb(\n",
        "    train_dataset=train_dataset,\n",
        "    val_dataset=val_dataset,\n",
        "    learning_rates=learning_rates,\n",
        "    batch_sizes=batch_sizes,\n",
        "    num_epochs=num_epochs\n",
        ")\n",
        "\n",
        "print(\"\\n=== Hyperparameter-Tuning abgeschlossen ===\")\n",
        "print(\"Alle Experimente wurden in wandb gespeichert!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualisierung der Hyperparameter-Ergebnisse\n",
        "print(\"=== Hyperparameter-Vergleich ===\")\n",
        "\n",
        "# Ergebnisse als DataFrame für bessere Übersicht\n",
        "import pandas as pd\n",
        "\n",
        "# Zusammenfassung der Ergebnisse\n",
        "summary_data = []\n",
        "for config_key, result in results.items():\n",
        "    summary_data.append({\n",
        "        'Learning Rate': result['learning_rate'],\n",
        "        'Batch Size': result['batch_size'],\n",
        "        'Final Train Acc (%)': result['final_train_acc'],\n",
        "        'Final Val Acc (%)': result['final_val_acc'],\n",
        "        'Final Train Loss': result['final_train_loss'],\n",
        "        'Final Val Loss': result['final_val_loss']\n",
        "    })\n",
        "\n",
        "df_summary = pd.DataFrame(summary_data)\n",
        "df_summary = df_summary.sort_values('Final Val Acc (%)', ascending=False)\n",
        "\n",
        "print(\"Ergebnisse sortiert nach Validation Accuracy:\")\n",
        "print(df_summary.to_string(index=False))\n",
        "\n",
        "# Beste Konfiguration finden\n",
        "best_config = df_summary.iloc[0]\n",
        "print(f\"\\nBeste Konfiguration:\")\n",
        "print(f\"Learning Rate: {best_config['Learning Rate']}\")\n",
        "print(f\"Batch Size: {best_config['Batch Size']}\")\n",
        "print(f\"Validation Accuracy: {best_config['Final Val Acc (%)']:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Lernkurven-Vergleich für verschiedene Konfigurationen\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Für jede Lernrate einen Subplot\n",
        "for i, lr in enumerate(learning_rates):\n",
        "    ax = axes[i]\n",
        "    \n",
        "    for batch_size in batch_sizes:\n",
        "        config_key = f\"LR_{lr}_Batch_{batch_size}\"\n",
        "        result = results[config_key]\n",
        "        \n",
        "        epochs = range(1, len(result['val_accs']) + 1)\n",
        "        ax.plot(epochs, result['val_accs'], \n",
        "               label=f'Batch {batch_size}', \n",
        "               linewidth=2, marker='o', markersize=4)\n",
        "    \n",
        "    ax.set_title(f'Learning Rate = {lr}')\n",
        "    ax.set_xlabel('Epoch')\n",
        "    ax.set_ylabel('Validation Accuracy (%)')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Für jede Batch-Größe einen Subplot\n",
        "for i, batch_size in enumerate(batch_sizes):\n",
        "    ax = axes[i + 3]\n",
        "    \n",
        "    for lr in learning_rates:\n",
        "        config_key = f\"LR_{lr}_Batch_{batch_size}\"\n",
        "        result = results[config_key]\n",
        "        \n",
        "        epochs = range(1, len(result['val_accs']) + 1)\n",
        "        ax.plot(epochs, result['val_accs'], \n",
        "               label=f'LR {lr}', \n",
        "               linewidth=2, marker='o', markersize=4)\n",
        "    \n",
        "    ax.set_title(f'Batch Size = {batch_size}')\n",
        "    ax.set_xlabel('Epoch')\n",
        "    ax.set_ylabel('Validation Accuracy (%)')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle('Hyperparameter-Tuning: Lernkurven-Vergleich', fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cross-Validation für statistische Fehlerschätzung\n",
        "\n",
        "**Ziel:** Schätzung des statistischen Fehlers der Metriken durch Cross-Validation.\n",
        "\n",
        "**Konfiguration:**\n",
        "- **5-Fold Cross-Validation** auf dem Trainingsdatensatz\n",
        "- **Beste Hyperparameter** aus dem Tuning verwenden\n",
        "- **Statistische Auswertung:** Mittelwert ± Standardabweichung\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cross-Validation für statistische Fehlerschätzung\n",
        "print(\"=== Cross-Validation ===\")\n",
        "\n",
        "# Beste Hyperparameter aus dem Tuning verwenden\n",
        "best_lr = best_config['Learning Rate']\n",
        "best_batch = int(best_config['Batch Size'])\n",
        "\n",
        "print(f\"Verwende beste Konfiguration: LR={best_lr}, Batch={best_batch}\")\n",
        "\n",
        "# Cross-Validation durchführen\n",
        "cv_results = cross_validation_training(\n",
        "    train_dataset=full_train_dataset,\n",
        "    num_folds=5,\n",
        "    num_epochs=50, \n",
        "    learning_rate=best_lr,\n",
        "    batch_size=best_batch\n",
        ")\n",
        "\n",
        "print(f\"\\n=== Cross-Validation Ergebnisse ===\")\n",
        "print(f\"Training Accuracy: {cv_results['train_accuracies']['mean']:.4f} ± {cv_results['train_accuracies']['std']:.4f}\")\n",
        "print(f\"Validation Accuracy: {cv_results['val_accuracies']['mean']:.4f} ± {cv_results['val_accuracies']['std']:.4f}\")\n",
        "print(f\"Training Loss: {cv_results['train_losses']['mean']:.4f} ± {cv_results['train_losses']['std']:.4f}\")\n",
        "print(f\"Validation Loss: {cv_results['val_losses']['mean']:.4f} ± {cv_results['val_losses']['std']:.4f}\")\n",
        "\n",
        "# Visualisierung der CV-Ergebnisse\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Accuracy Boxplot\n",
        "acc_data = [cv_results['train_accuracies']['values'], cv_results['val_accuracies']['values']]\n",
        "axes[0].boxplot(acc_data, labels=['Training', 'Validation'])\n",
        "axes[0].set_title('Cross-Validation: Accuracy Distribution')\n",
        "axes[0].set_ylabel('Accuracy (%)')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Loss Boxplot\n",
        "loss_data = [cv_results['train_losses']['values'], cv_results['val_losses']['values']]\n",
        "axes[1].boxplot(loss_data, labels=['Training', 'Validation'])\n",
        "axes[1].set_title('Cross-Validation: Loss Distribution')\n",
        "axes[1].set_ylabel('Loss')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Ergebnisse speichern\n",
        "import json\n",
        "with open('results/cross_validation_results.json', 'w') as f:\n",
        "    json.dump({\n",
        "        'best_hyperparameters': {\n",
        "            'learning_rate': best_lr,\n",
        "            'batch_size': best_batch\n",
        "        },\n",
        "        'cv_results': cv_results\n",
        "    }, f, indent=2)\n",
        "\n",
        "print(\"Cross-Validation Ergebnisse gespeichert als 'results/cross_validation_results.json'\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
